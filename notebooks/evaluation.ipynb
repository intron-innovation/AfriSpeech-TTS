{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVKWj1ZOSkJD"
   },
   "source": [
    "## TTS model evaluation (VITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POSw5dzUSkJE"
   },
   "outputs": [],
   "source": [
    "!pip install awscli\n",
    "!pip install --upgrade huggingface_hub\n",
    "\n",
    "!pip install 'transformers[torch]'\n",
    "!pip install 'datasets[audio]'\n",
    "\n",
    "!pip install --upgrade evaluate jiwer\n",
    "!pip install mel-cepstral-distance --user\n",
    "!pip install resemblyzer\n",
    "!pip install git+https://github.com/AndreevP/wvmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1c7OjxLSkJF"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from datasets import Dataset\n",
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    VitsModel,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG_Q5ZrkSkJF"
   },
   "source": [
    "### Dataset & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qc2YR8jSkJF"
   },
   "outputs": [],
   "source": [
    "# load csv files\n",
    "\n",
    "test_seen = pd.read_csv(\"https://raw.githubusercontent.com/intron-innovation/AfriSpeech-TTS/vits/data/afritts-test-seen-clean.csv\")\n",
    "test_unseen = pd.read_csv(\"https://raw.githubusercontent.com/intron-innovation/AfriSpeech-TTS/vits/data/afritts-test-unseen-clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luMyOfcDSkJF"
   },
   "outputs": [],
   "source": [
    "# Create a custom HF dataset\n",
    "\n",
    "# Convert DataFrame to a dictionary with lists\n",
    "test_seen['age_group'] = test_seen['age_group'].astype(str).fillna('null')\n",
    "test_seen_dict = test_seen.to_dict(orient='list')\n",
    "test_unseen_dict = test_unseen.to_dict(orient='list')\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "seen_dataset = Dataset.from_dict(test_seen_dict)\n",
    "unseen_dataset = Dataset.from_dict(test_unseen_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-Q-sgn1WFv3"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = \"../data/\" # change to your data root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTGkg_ZlWR22"
   },
   "outputs": [],
   "source": [
    "# Download the audio data (if not already downloaded)\n",
    "\n",
    "!aws configure\n",
    "!aws s3 cp s3://intron-open-source/AfriSpeech-TTS-D {DATA_ROOT}/AfriSpeech-TTS-D/ --recursive\n",
    "!aws s3 cp s3://intron-open-source/AfriSpeech-TTS/tts_generated_speech {DATA_ROOT}/AfriSpeech-TTS-D/tts_generated_speech/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87jWr8PuSkJF"
   },
   "outputs": [],
   "source": [
    "# Load TTS models and tokenizers\n",
    "\n",
    "def initialize_vits_model(model_id, device, torch_dtype='float32'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = VitsModel.from_pretrained(model_id, torch_dtype=torch_dtype, use_safetensors=True)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "\n",
    "# Initialize various VITS models\n",
    "#\n",
    "# Change \"facebook/mms-tts-eng\" model id to \"your_model_id\"\n",
    "#\n",
    "vits_vctk_model, vits_vctk_tokenizer = initialize_vits_model(\"facebook/mms-tts-eng\", device, torch_dtype) # your_model_id\n",
    "vits_afrotts_model, vits_afrotts_tokenizer = initialize_vits_model(\"facebook/mms-tts-eng\", device, torch_dtype) # your_model_id\n",
    "vits_afrotts_ft_model, vits_afrotts_ft_tokenizer = initialize_vits_model(\"facebook/mms-tts-eng\", device, torch_dtype) # your_model_id\n",
    "vits_afrotts_ft_ext_spk_model, vits_afrotts_ft_ext_spk_tokenizer = initialize_vits_model(\"facebook/mms-tts-eng\", device, torch_dtype) # your_model_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWTxs5moSkJG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Inference and Save Speech Utterances\n",
    "\n",
    "def synthesize_save(model, tokenizer, transcripts, audio_paths, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    synthesized_speech_paths = []\n",
    "    for i, (text, path) in enumerate(zip(transcripts, audio_paths)):\n",
    "        # Extract the file name from the audio path\n",
    "        file_name = os.path.basename(path)\n",
    "        # Create the file path for the synthesized speech file using the ground truth file name\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"tts generated speech file {file_path} already exists. Loading...\")\n",
    "            synthesized_speech_paths.append(file_path)\n",
    "        else:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            audio = outputs.waveform[0].numpy()\n",
    "            scipy.io.wavfile.write(file_path, rate=model.config.sampling_rate, data=audio)\n",
    "            synthesized_speech_paths.append(file_path)\n",
    "    return synthesized_speech_paths\n",
    "\n",
    "output_dir = f\"{DATA_ROOT}/AfriSpeech-TTS-D/tts_generated_speech/\"\n",
    "\n",
    "\n",
    "# Run for seen speakers dataset\n",
    "transcript_seen = [example['transcript'] for example in seen_dataset]\n",
    "audio_path_seen = [f\"{DATA_ROOT}\" + example['audio_paths'] for example in seen_dataset]\n",
    "vits_vctk_audio_path_seen = synthesize_save(vits_vctk_model, vits_vctk_tokenizer, transcript_seen, audio_path_seen, f\"{output_dir}/afritts_test_seen/vits_vctk\")\n",
    "vits_afrotts_audio_path_seen = synthesize_save(vits_afrotts_model, vits_afrotts_tokenizer, transcript_seen, audio_path_seen, f\"{output_dir}/afritts_test_seen/vits_afrotts\")\n",
    "vits_afrotts_ft_audio_path_seen = synthesize_save(vits_afrotts_ft_model, vits_afrotts_ft_tokenizer, transcript_seen, audio_path_seen, f\"{output_dir}/afritts_test_seen/vits_afrotts_ft\")\n",
    "vits_afrotts_ft_ext_spk_audio_path_seen = synthesize_save(vits_afrotts_ft_ext_spk_model, vits_afrotts_ft_ext_spk_tokenizer, transcript_seen, audio_path_seen, f\"{output_dir}/afritts_test_seen/vits_afrotts_ft_ext_spk\")\n",
    "\n",
    "# Run for unseen speakers dataset\n",
    "transcript_unseen = [example['transcript'] for example in unseen_dataset]\n",
    "audio_path_unseen = [f\"{DATA_ROOT}\" + example['audio_paths'] for example in unseen_dataset]\n",
    "vits_afrotts_ft_ext_spk_audio_path_unseen = synthesize_save(vits_afrotts_ft_ext_spk_model, vits_afrotts_ft_ext_spk_tokenizer, transcript_unseen, audio_path_unseen, f\"{output_dir}/afritts_untest_seen/vits_afrotts_ft_ext_spk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGscsn3OSkJG"
   },
   "outputs": [],
   "source": [
    "# explore what the speech utterances sound like\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def display_audio(audio_files, transcripts, indices):\n",
    "    with gr.Blocks() as demo:\n",
    "      with gr.Column():\n",
    "          for idx in indices:\n",
    "              audio, label = audio_files[idx], transcripts[idx]\n",
    "              output = gr.Audio(audio, label=label)\n",
    "    demo.launch(debug=False, share=True)\n",
    "\n",
    "# select 5 random examples\n",
    "indices = np.random.choice(len(audio_path_seen), 5, replace=False)\n",
    "\n",
    "print(\"ground_truth: \\n\")\n",
    "display_audio(audio_path_seen, transcript_seen, indices)\n",
    "\n",
    "print(\"vits_vctk: \\n\")\n",
    "display_audio(vits_vctk_audio_path_seen, transcript_seen, indices)\n",
    "\n",
    "print(\"vits_afrotts: \\n\")\n",
    "display_audio(vits_afrotts_audio_path_seen, transcript_seen, indices)\n",
    "\n",
    "print(\"vits_afrotts_ft: \\n\")\n",
    "display_audio(vits_afrotts_ft_audio_path_seen, transcript_seen, indices)\n",
    "\n",
    "print(\"vits_afrotts_ft_ext_spk: \\n\")\n",
    "display_audio(vits_afrotts_ft_ext_spk_audio_path_seen, transcript_seen, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19qdRQmESkJG",
    "tags": []
   },
   "source": [
    "### Word Error Rate (Intelligibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alukHsMeSkJG"
   },
   "outputs": [],
   "source": [
    "# Load ASR model\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "asr_model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "asr_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    asr_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True)\n",
    "asr_model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(asr_model_id)\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=asr_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yh9ypzSKSkJG",
    "outputId": "449c264e-6944-4c97-9cee-929d1c4550ae"
   },
   "outputs": [],
   "source": [
    "# Transcribe speech utterances using ASR model\n",
    "\n",
    "def transcribe(model, audio_files):\n",
    "    transcriptions = []\n",
    "    for audio_path in tqdm(audio_files, desc=\"Transcribing audio files\"):\n",
    "        audio, sampling_rate = librosa.load(audio_path)\n",
    "        result = model(audio)\n",
    "        transcriptions.append(result[\"text\"])\n",
    "    return transcriptions\n",
    "\n",
    "# Run for seen speakers dataset\n",
    "gt_transcripts_seen = transcribe(asr_pipe, audio_path_seen)\n",
    "vits_vctk_transcripts_seen = transcribe(asr_pipe, vits_vctk_audio_path_seen)\n",
    "vits_afrotts_transcripts_seen = transcribe(asr_pipe, vits_afrotts_audio_path_seen)\n",
    "vits_afrotts_ft_transcripts_seen = transcribe(asr_pipe, vits_afrotts_ft_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_transcripts_seen = transcribe(asr_pipe, vits_afrotts_ft_ext_spk_audio_path_seen)\n",
    "\n",
    "# Run for unseen speakers dataset\n",
    "gt_transcripts_unseen = transcribe(asr_pipe, audio_path_unseen)\n",
    "vits_afrotts_ft_ext_spk_transcripts_unseen = transcribe(asr_pipe, vits_afrotts_ft_ext_spk_audio_path_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MpYIGvcSkJG"
   },
   "outputs": [],
   "source": [
    "# Compute WER for the transcriptions\n",
    "\n",
    "import evaluate\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "def compute_normalized_wer(predictions, ground_truth):\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "    # Normalize predictions and ground truth\n",
    "    normalizer = BasicTextNormalizer()\n",
    "    predictions_norm = [normalizer(pred) for pred in predictions]\n",
    "    references_norm = [normalizer(label) for label in ground_truth]\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(references=references_norm, predictions=predictions_norm)\n",
    "\n",
    "    return wer\n",
    "\n",
    "# Run for seen speakers dataset\n",
    "gt_wer_seen = compute_normalized_wer(gt_transcripts_seen, transcript_seen)\n",
    "vits_vctk_wer_seen = compute_normalized_wer(vits_vctk_transcripts_seen, transcript_seen)\n",
    "vits_afrotts_wer_seen = compute_normalized_wer(vits_afrotts_transcripts_seen, transcript_seen)\n",
    "vits_afrotts_ft_wer_seen = compute_normalized_wer(vits_afrotts_ft_transcripts_seen, transcript_seen)\n",
    "vits_afrotts_ft_ext_spk_wer_seen = compute_normalized_wer(vits_afrotts_ft_ext_spk_transcripts_seen, transcript_seen)\n",
    "\n",
    "# Run for unseen speakers dataset\n",
    "gt_wer_unseen = compute_normalized_wer(gt_transcripts_unseen, transcript_unseen)\n",
    "vits_afrotts_ft_ext_spk_wer_unseen = compute_normalized_wer(vits_afrotts_ft_ext_spk_transcripts_unseen, transcript_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "VaEi0w1ISkJG",
    "outputId": "85a7afa3-e922-4e8d-9879-034946c750c4"
   },
   "outputs": [],
   "source": [
    "# Compile the results\n",
    "results = {\n",
    "    \"Model\": [\n",
    "        \"ground_truth\", \"vits_vctk\", \"vits_afrotts\", \"vits_afrotts_ft\", \"vits_afrotts_ft_ext_spk\", \"ground_truth\", \"vits_afrotts_ft_ext_spk\"\n",
    "    ],\n",
    "    \"Dataset\": [\n",
    "        \"seen speakers\", \"seen speakers\", \"seen speakers\", \"seen speakers\", \"seen speakers\", \"unseen speakers\", \"unseen speakers\"\n",
    "    ],\n",
    "    'WER': [gt_wer_seen, vits_vctk_wer_seen, vits_afrotts_wer_seen, vits_afrotts_ft_wer_seen, vits_afrotts_ft_ext_spk_wer_seen, gt_wer_unseen, vits_afrotts_ft_ext_spk_wer_unseen],\n",
    "}\n",
    "wer_results_df = pd.DataFrame(results)\n",
    "display(wer_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtba-YHWSkJG"
   },
   "source": [
    "### Mel-Cepstral-Distance (Speech Signal Similarity)\n",
    "https://github.com/jasminsternkopf/mel_cepstral_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PchQamhJSkJG"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mel_cepstral_distance import get_metrics_wavs\n",
    "\n",
    "def compute_mcd(ref_audio_path, synth_audio_path):\n",
    "    mcd_arr = []\n",
    "    for ref, synth in zip(ref_audio_path, synth_audio_path):\n",
    "        mcd_audio, _, _ = get_metrics_wavs(Path(ref), Path(synth), use_dtw=False)\n",
    "        mcd_arr.append(mcd_audio)\n",
    "    return mcd_arr\n",
    "\n",
    "\n",
    "# Compute MCD\n",
    "vits_afrotts_mcd_seen = compute_mcd(audio_path_seen, vits_afrotts_audio_path_seen)\n",
    "vits_afrotts_ft_mcd_seen = compute_mcd(audio_path_seen, vits_afrotts_ft_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_mcd_seen = compute_mcd(audio_path_seen, vits_afrotts_ft_ext_spk_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_mcd_unseen = compute_mcd(audio_path_unseen, vits_afrotts_ft_ext_spk_audio_path_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2wsHL6mSkJG"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# function to compute confidence interval\n",
    "\n",
    "def compute_confidence_interval(score_arr, ci=0.95):\n",
    "    # computes the CI at 95 perc confidence level\n",
    "\n",
    "    # Filter out NaN values\n",
    "    score_arr = np.array(score_arr)\n",
    "    clean_arr = score_arr[~np.isnan(score_arr)]\n",
    "    mean_score = np.nanmean(clean_arr)\n",
    "    ci = st.t.interval(\n",
    "            confidence=ci,\n",
    "            df=len(clean_arr) - 1,\n",
    "            loc=mean_score,\n",
    "            scale=st.sem(clean_arr) if np.std(clean_arr) > 0 else 0,\n",
    "        )\n",
    "\n",
    "    return mean_score, mean_score-ci[0]\n",
    "\n",
    "\n",
    "# Compute the mean and confidence interval for each MCD array\n",
    "mean_ci_vits_afrotts_seen, ci_vits_afrotts_seen = compute_confidence_interval(vits_afrotts_mcd_seen)\n",
    "mean_ci_vits_afrotts_ft_seen, ci_vits_afrotts_ft_seen = compute_confidence_interval(vits_afrotts_ft_mcd_seen)\n",
    "mean_ci_vits_afrotts_ft_ext_spk_seen, ci_vits_afrotts_ft_ext_spk_seen = compute_confidence_interval(vits_afrotts_ft_ext_spk_mcd_seen)\n",
    "mean_ci_vits_afrotts_ft_ext_spk_unseen, ci_vits_afrotts_ft_ext_spk_unseen = compute_confidence_interval(vits_afrotts_ft_ext_spk_mcd_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "GHzXFEa6SkJG",
    "outputId": "22d58b23-dba7-4146-ee4d-6fd0bc95ab6f"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Model': [\n",
    "        'vits_afrotts', 'vits_afrotts_ft', 'vits_afrotts_ft_ext_spk', 'vits_afrotts_ft_ext_spk'\n",
    "    ],\n",
    "    'Dataset': [\n",
    "        'seen speakers', 'seen speakers', 'seen speakers', 'unseen speakers'\n",
    "    ],\n",
    "    'Mean MCD': [\n",
    "        mean_ci_vits_afrotts_seen,\n",
    "        mean_ci_vits_afrotts_ft_seen,\n",
    "        mean_ci_vits_afrotts_ft_ext_spk_seen,\n",
    "        mean_ci_vits_afrotts_ft_ext_spk_unseen\n",
    "    ],\n",
    "    'Confidence Interval': [\n",
    "        ci_vits_afrotts_seen,\n",
    "        ci_vits_afrotts_ft_seen,\n",
    "        ci_vits_afrotts_ft_ext_spk_seen,\n",
    "        ci_vits_afrotts_ft_ext_spk_unseen\n",
    "    ]\n",
    "}\n",
    "mcd_ci_df = pd.DataFrame(data)\n",
    "display(mcd_ci_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zoe3gRbrSkJG"
   },
   "source": [
    "### Cosine Distance (Speaker Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJkQgWkLSkJH",
    "outputId": "0d3961e7-fea0-48fc-fca1-1b1ce2925bb1"
   },
   "outputs": [],
   "source": [
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "encoder = VoiceEncoder()\n",
    "\n",
    "def compute_cos_sim(encoder, ref_audio_path, synth_audio_path):\n",
    "    cos_sim_arr = []\n",
    "    for ref, synth in zip(ref_audio_path, synth_audio_path):\n",
    "            ref_wav = preprocess_wav(Path(ref))\n",
    "            gen_wav = preprocess_wav(Path(synth))\n",
    "\n",
    "            ref_emb = encoder.embed_utterance(ref_wav)\n",
    "            gen_emb = encoder.embed_utterance(gen_wav)\n",
    "\n",
    "            # the embeddings are already l2 normalized by the speaker model\n",
    "            cos_sim = ref_emb @ gen_emb\n",
    "\n",
    "            cos_sim_arr.append(cos_sim)\n",
    "    return cos_sim_arr\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "vits_afrotts_cos_sim_seen = compute_cos_sim(encoder, audio_path_seen, vits_afrotts_audio_path_seen)\n",
    "vits_afrotts_ft_cos_sim_seen = compute_cos_sim(encoder, audio_path_seen, vits_afrotts_ft_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_cos_sim_seen = compute_cos_sim(encoder, audio_path_seen, vits_afrotts_ft_ext_spk_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_cos_sim_unseen = compute_cos_sim(encoder, audio_path_unseen, vits_afrotts_ft_ext_spk_audio_path_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThpvSrQWSkJH"
   },
   "outputs": [],
   "source": [
    "# Compute mean and confidence interval for each array\n",
    "mean_ci_afrotts, ci_afrotts = compute_confidence_interval(vits_afrotts_cos_sim_seen)\n",
    "mean_ci_afrotts_ft, ci_afrotts_ft = compute_confidence_interval(vits_afrotts_ft_cos_sim_seen)\n",
    "mean_ci_afrotts_ft_ext_spk_seen, ci_afrotts_ft_ext_spk_seen = compute_confidence_interval(vits_afrotts_ft_ext_spk_cos_sim_seen)\n",
    "mean_ci_afrotts_ft_ext_spk_unseen, ci_afrotts_ft_ext_spk_unseen = compute_confidence_interval(vits_afrotts_ft_ext_spk_cos_sim_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "Z4BD0NVVSkJH",
    "outputId": "f9ec6056-725f-4721-c146-c4982aaea688"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Model': [\n",
    "        'vits_afrotts', 'vits_afrotts_ft', 'vits_afrotts_ft_ext_spk', 'vits_afrotts_ft_ext_spk'\n",
    "    ],\n",
    "    'Dataset': [\n",
    "        'seen speakers', 'seen speakers', 'seen speakers', 'unseen speakers'\n",
    "    ],\n",
    "    'Mean Cosine Similarity': [\n",
    "        mean_ci_afrotts, mean_ci_afrotts_ft, mean_ci_afrotts_ft_ext_spk_seen, mean_ci_afrotts_ft_ext_spk_unseen\n",
    "    ],\n",
    "    'Confidence Interval': [\n",
    "        ci_afrotts, ci_afrotts_ft, ci_afrotts_ft_ext_spk_seen, ci_afrotts_ft_ext_spk_unseen\n",
    "    ]\n",
    "}\n",
    "cos_sim_results_df = pd.DataFrame(data)\n",
    "display(cos_sim_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eqTsi35SkJH"
   },
   "source": [
    "### WV-MOS (Overall quality)\n",
    "\n",
    "https://github.com/AndreevP/wvmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sctPhDevSkJH"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# you need a gpu to load the model\n",
    "# =======================\n",
    "from wvmos import get_wvmos\n",
    "\n",
    "wvmos_model = get_wvmos(cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azcetvzYpb9J"
   },
   "outputs": [],
   "source": [
    "def compute_mos_scores(audio_paths, model):\n",
    "    mos_scores = []\n",
    "    for wav_file_path in audio_paths:\n",
    "        # Infer MOS score for one audio file\n",
    "        mos_score = model.calculate_one(wav_file_path)\n",
    "        mos_scores.append(mos_score)\n",
    "\n",
    "    return mos_scores\n",
    "\n",
    "# Run for seen speakers dataset\n",
    "gt_mos_seen = compute_mos_scores(audio_path_seen, wvmos_model)\n",
    "vits_vctk_mos_seen = compute_mos_scores(vits_vctk_audio_path_seen, wvmos_model)\n",
    "vits_afrotts_mos_seen = compute_mos_scores(vits_afrotts_audio_path_seen, wvmos_model)\n",
    "vits_afrotts_ft_mos_seen = compute_mos_scores(vits_afrotts_ft_audio_path_seen, wvmos_model)\n",
    "vits_afrotts_ft_ext_spk_mos_seen = compute_mos_scores(vits_afrotts_ft_ext_spk_audio_path_seen, wvmos_model)\n",
    "\n",
    "# Run for unseen speakers dataset\n",
    "gt_mos_unseen = compute_mos_scores(audio_path_unseen, wvmos_model)\n",
    "vits_afrotts_ft_ext_spk_mos_unseen = compute_mos_scores(vits_afrotts_ft_ext_spk_audio_path_unseen, wvmos_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyZ39Vb6uQ2F"
   },
   "outputs": [],
   "source": [
    "# Compute mean score and confidence interval for the seen dataset\n",
    "mean_gt_mos_seen, ci_gt_mos_seen = compute_confidence_interval(gt_mos_seen)\n",
    "mean_vits_vctk_mos_seen, ci_vits_vctk_mos_seen = compute_confidence_interval(vits_vctk_mos_seen)\n",
    "mean_vits_afrotts_mos_seen, ci_vits_afrotts_mos_seen = compute_confidence_interval(vits_afrotts_mos_seen)\n",
    "mean_vits_afrotts_ft_mos_seen, ci_vits_afrotts_ft_mos_seen = compute_confidence_interval(vits_afrotts_ft_mos_seen)\n",
    "mean_vits_afrotts_ft_ext_spk_mos_seen, ci_vits_afrotts_ft_ext_spk_mos_seen = compute_confidence_interval(vits_afrotts_ft_ext_spk_mos_seen)\n",
    "\n",
    "# Compute mean score and confidence interval for the unseen dataset\n",
    "mean_gt_mos_unseen, ci_gt_mos_unseen = compute_confidence_interval(gt_mos_unseen)\n",
    "mean_vits_afrotts_ft_ext_spk_mos_unseen, ci_vits_afrotts_ft_ext_spk_mos_unseen = compute_confidence_interval(vits_afrotts_ft_ext_spk_mos_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "YwbIruimSkJH",
    "outputId": "34634864-7122-4410-a42e-91175da361f1"
   },
   "outputs": [],
   "source": [
    "# compile the results\n",
    "data = {\n",
    "    \"Model\": [\n",
    "        \"ground_truth\", \"vits_vctk\", \"vits_afrotts\", \"vits_afrotts_ft\", \"vits_afrotts_ft_ext_spk\", \"ground_truth\", \"vits_afrotts_ft_ext_spk\"\n",
    "    ],\n",
    "    \"Dataset\": [\n",
    "        \"seen speakers\", \"seen speakers\", \"seen speakers\", \"seen speakers\", \"seen speakers\", \"unseen speakers\", \"unseen speakers\"\n",
    "    ],\n",
    "    \"Mean WV-MOS Score\": [\n",
    "        mean_gt_mos_seen, mean_vits_vctk_mos_seen, mean_vits_afrotts_mos_seen,\n",
    "        mean_vits_afrotts_ft_mos_seen, mean_vits_afrotts_ft_ext_spk_mos_seen,\n",
    "        mean_gt_mos_unseen, mean_vits_afrotts_ft_ext_spk_mos_unseen\n",
    "    ],\n",
    "    \"Confidence Interval\": [\n",
    "        ci_gt_mos_seen, ci_vits_vctk_mos_seen, ci_vits_afrotts_mos_seen,\n",
    "        ci_vits_afrotts_ft_mos_seen, ci_vits_afrotts_ft_ext_spk_mos_seen,\n",
    "        ci_gt_mos_unseen, ci_vits_afrotts_ft_ext_spk_mos_unseen\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(data)\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZN5WuwZSkJH"
   },
   "source": [
    "### check if a TTSmodel is statistically better than another TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VLq8aAH2snM",
    "outputId": "3cc6341d-525e-4e96-8e11-a31f175c0c90"
   },
   "outputs": [],
   "source": [
    "# verify that model 1 is better than model 2 in WV-mos scores\n",
    "\n",
    "def compare_models(mos_scores_model1, mos_scores_model2, ci_level=0.95):\n",
    "    # Compute difference in scores\n",
    "    diff_in_scores = np.array(mos_scores_model1) - np.array(mos_scores_model2)\n",
    "\n",
    "    # Compute the confidence interval of the difference\n",
    "    mean_score, ci_half_width = compute_confidence_interval(diff_in_scores, ci=ci_level)\n",
    "    lower_bound = mean_score - ci_half_width\n",
    "    upper_bound = mean_score + ci_half_width\n",
    "\n",
    "    # If the confidence intervals lie fully on the positive side on the real axis,\n",
    "    # this means that the difference is statistically significant.\n",
    "    # E.g., for WV-MOS, the confidence interval will be 0.14 +/- 0.xx. If xx is smaller than 14,\n",
    "    # then the difference is statistically significant.\n",
    "\n",
    "    # Check if the confidence interval lies fully on the positive side\n",
    "    if lower_bound > 0:\n",
    "        print(\"Model 1 is statistically significantly better than Model 2\")\n",
    "    elif upper_bound < 0:\n",
    "        print(\"Model 2 is statistically significantly better than Model 1\")\n",
    "    else:\n",
    "        print(\"No statistically significant difference between Model 1 and Model 2\")\n",
    "\n",
    "\n",
    "# Compare VITS VCTK vs VITS Afrotts on seen dataset\n",
    "print(\"Comparing VITS VCTK vs VITS Afrotts on seen dataset:\")\n",
    "compare_models(vits_vctk_mos_seen, vits_afrotts_mos_seen)\n",
    "\n",
    "# Compare VITS VCTK vs VITS Afrotts FT on seen dataset\n",
    "print(\"\\nComparing VITS VCTK vs VITS Afrotts FT on seen dataset:\")\n",
    "compare_models(vits_vctk_mos_seen, vits_afrotts_ft_mos_seen)\n",
    "\n",
    "# Compare VITS VCTK vs VITS Afrotts FT EXT SPK on seen dataset\n",
    "print(\"\\nComparing VITS VCTK vs VITS Afrotts FT EXT SPK on seen dataset:\")\n",
    "compare_models(vits_vctk_mos_seen, vits_afrotts_ft_ext_spk_mos_seen)\n",
    "\n",
    "# Compare VITS Afrotts vs VITS Afrotts FT on seen dataset\n",
    "print(\"\\nComparing VITS Afrotts vs VITS Afrotts FT on seen dataset:\")\n",
    "compare_models(vits_afrotts_mos_seen, vits_afrotts_ft_mos_seen)\n",
    "\n",
    "# Compare VITS Afrotts vs VITS Afrotts FT EXT SPK on seen dataset\n",
    "print(\"\\nComparing VITS Afrotts vs VITS Afrotts FT EXT SPK on seen dataset:\")\n",
    "compare_models(vits_afrotts_mos_seen, vits_afrotts_ft_ext_spk_mos_seen)\n",
    "\n",
    "# Compare VITS Afrotts FT vs VITS Afrotts FT EXT SPK on seen dataset\n",
    "print(\"\\nComparing VITS Afrotts FT vs VITS Afrotts FT EXT SPK on seen dataset:\")\n",
    "compare_models(vits_afrotts_ft_mos_seen, vits_afrotts_ft_ext_spk_mos_seen)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
