{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to evaluate TTS models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv files\n",
    "\n",
    "# test_seen = pd.read_csv(\"/AfriSpeech-TTS/data/afritts-test-seen-clean.csv\")\n",
    "# test_unseen = pd.read_csv(\"/AfriSpeech-TTS/data/afritts-test-unseen-clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute confidence interval\n",
    "\n",
    "def compute_confidence_interval(score_arr, ci=0.95):\n",
    "    # computes the CI at 95 perc confidence level\n",
    "    \n",
    "    mean_score = np.mean(score_arr)\n",
    "    ci = st.t.interval(\n",
    "            alpha=0.95,\n",
    "            df=len(score_arr) - 1,\n",
    "            loc=mean_score,\n",
    "            scale=st.sem(score_arr),\n",
    "        )\n",
    "    \n",
    "    return mean_score, mean_score-ci[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mel-cepstral distance \n",
    "\n",
    "https://github.com/jasminsternkopf/mel_cepstral_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mel-cepstral-distance --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mel_cepstral_distance import get_metrics_wavs, get_metrics_mels, get_metrics_mels_pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcd_arr = []\n",
    "\n",
    "for wav_file_ref, wav_file_tts in path_to_wavs:\n",
    "    mcd_audio, _, _ = get_metrics_wavs(wav_file_ref, wav_file_tts,)\n",
    "    \n",
    "    mcd_arr.append(mcd_audio)\n",
    "    \n",
    "\n",
    "mean_mcd, ci_mcd = compute_confidence_interval(mcd_arr)\n",
    "#report the values, mean +/- ci\n",
    "print(f\"model mcd score: {mean_mcd} + {ci_mcd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wv-mos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/AndreevP/wvmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need a gpu to load the model\n",
    "from wvmos import get_wvmos\n",
    "\n",
    "wvmos_model = get_wvmos(cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mos_array = []\n",
    "\n",
    "for _, wav_file_tts in path_to_wavs:\n",
    "    mos_audio = wvmos_model.calculate_one(wav_file_tts) # infer MOS score for one audio\n",
    "    \n",
    "    mos_array.append(mos_audio)\n",
    "    \n",
    "mean_mos, ci_mos = compute_confidence_interval(mos_array)\n",
    "#report the values, mean +/- ci\n",
    "print(f\"model mcd score: {mean_mos} + {ci_mos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install resemblyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "encoder = VoiceEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_arr = [] \n",
    "\n",
    "for wav_file_ref, wav_file_tts in path_to_wavs:\n",
    "        ref_wav = preprocess_wav(Path(wav_file_ref))\n",
    "        gen_wav = preprocess_wav(Path(wav_file_tts))\n",
    "        \n",
    "        ref_emb = encoder.embed_utterance(ref_wav)\n",
    "        gen_emb = encoder.embed_utterance(gen_wav)\n",
    "        \n",
    "        # the embeddings are already l2 normalized by the speaker model\n",
    "        cos_sim = ref_emb @ gen_emb\n",
    "        \n",
    "        cos_sim_arr.append(cos_sim)\n",
    "        \n",
    "mean_cos_sim, ci_cos_sim = compute_confidence_interval(cos_sim_arr)\n",
    "#report the values, mean +/- ci\n",
    "print(f\"model mcd score: {mean_cos_sim} + {ci_cos_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### WER: need a whisper model trained on african accent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"our_finetuned_whisper_model\" #open_ai/whisper-medium-general\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "# create a custom dataset\n",
    "# compute WER\n",
    "\n",
    "# dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "sample = dataset[0][\"audio\"]\n",
    "\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
