{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS model evaluation (VITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'transformers[torch]'\n",
    "# !pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules \n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from datasets import Dataset\n",
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    VitsModel,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv files\n",
    "\n",
    "DATA_ROOT = \"../data\"\n",
    "test_seen = pd.read_csv(f\"{DATA_ROOT}/afritts-test-seen-clean.csv\")\n",
    "test_unseen = pd.read_csv(f\"{DATA_ROOT}/afritts-test-unseen-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom HF dataset\n",
    "\n",
    "# Convert DataFrame to a dictionary with lists\n",
    "test_seen['age_group'] = test_seen['age_group'].astype(str).fillna('null')\n",
    "test_seen_dict = test_seen.to_dict(orient='list')\n",
    "test_unseen_dict = test_unseen.to_dict(orient='list')\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "seen_dataset = Dataset.from_dict(test_seen_dict)\n",
    "unseen_dataset = Dataset.from_dict(test_unseen_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the seen dataset\n",
    "seen_dataset_selected = seen_dataset.filter(lambda example: example['gender'] in ['Male', 'Female']).select(range(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TTS models and tokenizers\n",
    "\n",
    "def initialize_vits_model(model_id, device, torch_dtype='float32'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = VitsModel.from_pretrained(model_id, torch_dtype=torch_dtype, use_safetensors=True)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "\n",
    "# Initialize various VITS models\n",
    "vits_vctk_model, vits_vctk_tokenizer = initialize_vits_model(\"facebook/mms-tts-eng\", device, torch_dtype) # your_finetune_model_id\n",
    "vits_afrotts_model, vits_afrotts_tokenizer = initialize_vits_model(\"facebook/mms-tts-eng\", device, torch_dtype) # your_finetune_model_id\n",
    "vits_afrotts_ft_model, vits_afrotts_ft_tokenizer = initialize_vits_model(\"facebook/mms-tts-eng\", device, torch_dtype) # your_finetune_model_id\n",
    "vits_afrotts_ft_ext_spk_model, vits_afrotts_ft_ext_spk_tokenizer = initialize_vits_model(\"facebook/mms-tts-eng\", device, torch_dtype) # your_finetune_model_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Inference and Save Speech Utterances\n",
    "\n",
    "def synthesize_save(model, tokenizer, transcripts, audio_paths, output_dir):\n",
    "    set_seed(555)  # make deterministic\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    synthesized_speech = []\n",
    "    for i, (text, path) in enumerate(zip(transcripts, audio_paths)):\n",
    "        # Extract the file name from the ground truth path\n",
    "        gt_file_name = os.path.basename(path)\n",
    "        # Create the file path for the synthesized file using the ground truth file name\n",
    "        file_path = os.path.join(output_dir, gt_file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"File {file_path} already exists. Loading...\")\n",
    "            synthesized_speech.append(file_path)\n",
    "        else:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            audio = outputs.waveform[0].numpy()\n",
    "            scipy.io.wavfile.write(file_path, rate=model.config.sampling_rate, data=audio)\n",
    "            synthesized_speech.append(file_path)\n",
    "    return synthesized_speech\n",
    "\n",
    "output_dir = f\"{DATA_ROOT}/AfriSpeech-TTS-D/tts_generated_speech/\"\n",
    "\n",
    "\n",
    "# Run for seen dataset \n",
    "transcript_seen = [example['transcript'] for example in seen_dataset]\n",
    "audio_path_seen = [f\"{DATA_ROOT}\" + example['audio_paths'] for example in seen_dataset]\n",
    "vits_vctk_audio_path_seen = synthesize_save(vits_vctk_model, vits_vctk_tokenizer, transcript_seen, audio_path_seen, f\"{output_dir}/afritts_test_seen/vits_vctk\")\n",
    "vits_afrotts_audio_path_seen = synthesize_save(vits_afrotts_model, vits_afrotts_tokenizer, transcript_seen, audio_path_seen, f\"{output_dir}/afritts_test_seen/vits_afrotts\")\n",
    "vits_afrotts_ft_audio_path_seen = synthesize_save(vits_afrotts_ft_model, vits_afrotts_ft_tokenizer, transcript_seen, audio_path_seen, f\"{output_dir}/afritts_test_seen/vits_afrotts_ft\")\n",
    "vits_afrotts_ft_ext_spk_audio_path_seen = synthesize_save(vits_afrotts_ft_ext_spk_model, vits_afrotts_ft_ext_spk_tokenizer, transcript_seen, audio_path_seen, f\"{output_dir}/afritts_test_seen/vits_afrotts_ft_ext_spk\")\n",
    "\n",
    "# Run for unseen dataset\n",
    "transcript_unseen = [example['transcript'] for example in unseen_dataset]\n",
    "audio_path_unseen = [f\"{DATA_ROOT}\" + example['audio_paths'] for example in unseen_dataset]\n",
    "vits_afrotts_ft_ext_spk_audio_path_unseen = synthesize_save(vits_afrotts_ft_ext_spk_model, vits_afrotts_ft_ext_spk_tokenizer, transcript_unseen, audio_path_unseen, f\"{output_dir}/afritts_untest_seen/vits_afrotts_ft_ext_spk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# explore what the speech utterances sound like \n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def display_audio(audio_files, transcripts, idx):\n",
    "    with gr.Blocks() as demo:\n",
    "      with gr.Column():\n",
    "          audio, label = audio_files[idx], transcripts[idx]\n",
    "          output = gr.Audio(audio, label=label)    \n",
    "    demo.launch(debug=False)\n",
    "\n",
    "# select a random example\n",
    "idx = np.random.randint(0, len(audio_path_seen))\n",
    "\n",
    "print(\"ground_truth: \\n\")\n",
    "display_audio(audio_path_seen, transcript_seen, idx)\n",
    "\n",
    "print(\"vits_vctk: \\n\")\n",
    "display_audio(vits_vctk_audio_path_seen, transcript_seen, idx)\n",
    "\n",
    "print(\"vits_afrotts: \\n\")\n",
    "display_audio(vits_afrotts_audio_path_seen, transcript_seen, idx)\n",
    "\n",
    "print(\"vits_afrotts_ft: \\n\")\n",
    "display_audio(vits_afrotts_ft_audio_path_seen, transcript_seen, idx)\n",
    "\n",
    "print(\"vits_afrotts_ft_ext_spk: \\n\")\n",
    "display_audio(vits_afrotts_ft_ext_spk_audio_path_seen, transcript_seen, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word Error Rate (Intelligibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ASR model\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "asr_model_id = \"Seyfelislem/afrispeech_large_A100\"\n",
    "\n",
    "asr_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    asr_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True)\n",
    "asr_model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=asr_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe speech utterances using ASR model\n",
    "\n",
    "def transcribe(model, audio_files):\n",
    "    transcriptions = []\n",
    "    for audio_path in tqdm(audio_files, desc=\"Transcribing audio files\"):\n",
    "        audio, sampling_rate = librosa.load(audio_path)\n",
    "        result = model(audio)\n",
    "        transcriptions.append(result[\"text\"])\n",
    "    return transcriptions\n",
    "\n",
    "# Run for seen dataset\n",
    "gt_transcripts_seen = transcribe(asr_pipe, audio_path_seen)\n",
    "vits_vctk_transcripts_seen = transcribe(asr_pipe, vits_vctk_audio_path_seen)\n",
    "vits_afrotts_transcripts_seen = transcribe(asr_pipe, vits_afrotts_audio_path_seen)\n",
    "vits_afrotts_ft_transcripts_seen = transcribe(asr_pipe, vits_afrotts_ft_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_transcripts_seen = transcribe(asr_pipe, vits_afrotts_ft_ext_spk_audio_path_seen)\n",
    "\n",
    "# Run for unseen dataset\n",
    "gt_transcripts_unseen = transcribe(asr_pipe, audio_path_unseen)\n",
    "vits_afrotts_ft_ext_spk_transcripts_unseen = transcribe(asr_pipe, vits_afrotts_ft_ext_spk_audio_path_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute WER for the transcriptions\n",
    "\n",
    "import evaluate\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "def compute_normalized_wer(predictions, ground_truth):\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    \n",
    "    # Normalize predictions and ground truth\n",
    "    normalizer = BasicTextNormalizer()\n",
    "    predictions_norm = [normalizer(pred) for pred in predictions]\n",
    "    references_norm = [normalizer(label) for label in ground_truth]\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(references=references_norm, predictions=predictions_norm)\n",
    "\n",
    "    return wer\n",
    "\n",
    "# Run for seen dataset\n",
    "gt_wer_seen = compute_normalized_wer(gt_transcripts_seen, transcript_seen)\n",
    "vits_vctk_wer_seen = compute_normalized_wer(vits_vctk_transcripts_seen, transcript_seen)\n",
    "vits_afrotts_wer_seen = compute_normalized_wer(vits_afrotts_transcripts_seen, transcript_seen)\n",
    "vits_afrotts_ft_wer_seen = compute_normalized_wer(vits_afrotts_ft_transcripts_seen, transcript_seen)\n",
    "vits_afrotts_ft_ext_spk_wer_seen = compute_normalized_wer(vits_afrotts_ft_ext_spk_transcripts_seen, transcript_seen)\n",
    "\n",
    "# Run for unseen dataset\n",
    "gt_wer_unseen = compute_normalized_wer(gt_transcripts_unseen, transcript_unseen)\n",
    "vits_afrotts_ft_ext_spk_wer_unseen = compute_normalized_wer(vits_afrotts_ft_ext_spk_transcripts_unseen, transcript_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the results\n",
    "results = {\n",
    "    'Model': ['Ground Truth', 'VITS VCTK', 'VITS AfriTTS', 'VITS AfriTTS FT', 'VITS AfriTTS FT EXT SPK'],\n",
    "    'WER Seen': [gt_wer_seen, vits_vctk_wer_seen, vits_afrotts_wer_seen, vits_afrotts_ft_wer_seen, vits_afrotts_ft_ext_spk_wer_seen],\n",
    "    'WER Unseen': [gt_wer_unseen, None, None, None, vits_afrotts_ft_ext_spk_wer_unseen]\n",
    "}\n",
    "wer_results_df = pd.DataFrame(results)\n",
    "display(wer_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-Cepstral-Distance (Speech Signal Similarity)\n",
    "https://github.com/jasminsternkopf/mel_cepstral_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mel-cepstral-distance --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mel_cepstral_distance import get_metrics_wavs\n",
    "\n",
    "def compute_mcd(ref_audio_path, synth_audio_path):\n",
    "    mcd_arr = []\n",
    "    for ref, synth in zip(ref_audio_path, synth_audio_path):\n",
    "        mcd_audio, _, _ = get_metrics_wavs(Path(ref), Path(synth), use_dtw=False)\n",
    "        mcd_arr.append(mcd_audio)\n",
    "    return mcd_arr\n",
    "    \n",
    "\n",
    "# Compute MCD\n",
    "vits_afrotts_mcd_seen = compute_mcd(audio_path_seen, vits_afrotts_audio_path_seen)\n",
    "vits_afrotts_ft_mcd_seen = compute_mcd(audio_path_seen, vits_afrotts_ft_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_mcd_seen = compute_mcd(audio_path_seen, vits_afrotts_ft_ext_spk_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_mcd_unseen = compute_mcd(audio_path_unseen, vits_afrotts_ft_ext_spk_audio_path_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# function to compute confidence interval\n",
    "\n",
    "def compute_confidence_interval(score_arr, ci=0.95):\n",
    "    # computes the CI at 95 perc confidence level\n",
    "\n",
    "    # Filter out NaN values\n",
    "    score_arr = np.array(score_arr)\n",
    "    clean_arr = score_arr[~np.isnan(score_arr)]\n",
    "    mean_score = np.nanmean(clean_arr)\n",
    "    ci = st.t.interval(\n",
    "            confidence=ci,\n",
    "            df=len(clean_arr) - 1,\n",
    "            loc=mean_score,\n",
    "            scale=st.sem(clean_arr) if np.std(clean_arr) > 0 else 0,\n",
    "        )\n",
    "    \n",
    "    return mean_score, mean_score-ci[0]\n",
    "\n",
    "\n",
    "# Compute the mean and confidence interval for each MCD array\n",
    "mean_ci_vits_afrotts_seen, ci_vits_afrotts_seen = compute_confidence_interval(vits_afrotts_mcd_seen)\n",
    "mean_ci_vits_afrotts_ft_seen, ci_vits_afrotts_ft_seen = compute_confidence_interval(vits_afrotts_ft_mcd_seen)\n",
    "mean_ci_vits_afrotts_ft_ext_spk_seen, ci_vits_afrotts_ft_ext_spk_seen = compute_confidence_interval(vits_afrotts_ft_ext_spk_mcd_seen)\n",
    "mean_ci_vits_afrotts_ft_ext_spk_unseen, ci_vits_afrotts_ft_ext_spk_unseen = compute_confidence_interval(vits_afrotts_ft_ext_spk_mcd_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Model': [\n",
    "        'VITS AfriTTS Seen',\n",
    "        'VITS AfriTTS FT Seen',\n",
    "        'VITS AfriTTS FT EXT SPK Seen',\n",
    "        'VITS AfriTTS FT EXT SPK Unseen'\n",
    "    ],\n",
    "    'Mean MCD': [\n",
    "        mean_ci_vits_afrotts_seen,\n",
    "        mean_ci_vits_afrotts_ft_seen,\n",
    "        mean_ci_vits_afrotts_ft_ext_spk_seen,\n",
    "        mean_ci_vits_afrotts_ft_ext_spk_unseen\n",
    "    ],\n",
    "    'Confidence Interval': [\n",
    "        ci_vits_afrotts_seen,\n",
    "        ci_vits_afrotts_ft_seen,\n",
    "        ci_vits_afrotts_ft_ext_spk_seen,\n",
    "        ci_vits_afrotts_ft_ext_spk_unseen\n",
    "    ]\n",
    "}\n",
    "mcd_ci_df = pd.DataFrame(data)\n",
    "display(mcd_ci_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance (Speaker Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install resemblyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "encoder = VoiceEncoder()\n",
    "\n",
    "def compute_cos_sim(encoder, ref_audio_path, synth_audio_path):\n",
    "    cos_sim_arr = [] \n",
    "    for ref, synth in zip(ref_audio_path, synth_audio_path):\n",
    "            ref_wav = preprocess_wav(Path(ref))\n",
    "            gen_wav = preprocess_wav(Path(synth))\n",
    "            \n",
    "            ref_emb = encoder.embed_utterance(ref_wav)\n",
    "            gen_emb = encoder.embed_utterance(gen_wav)\n",
    "            \n",
    "            # the embeddings are already l2 normalized by the speaker model\n",
    "            cos_sim = ref_emb @ gen_emb\n",
    "            \n",
    "            cos_sim_arr.append(cos_sim)\n",
    "    return cos_sim_arr\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "vits_afrotts_cos_sim_seen = compute_cos_sim(encoder, audio_path_seen, vits_afrotts_audio_path_seen)\n",
    "vits_afrotts_ft_cos_sim_seen = compute_cos_sim(encoder, audio_path_seen, vits_afrotts_ft_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_cos_sim_seen = compute_cos_sim(encoder, audio_path_seen, vits_afrotts_ft_ext_spk_audio_path_seen)\n",
    "vits_afrotts_ft_ext_spk_cos_sim_unseen = compute_cos_sim(encoder, audio_path_unseen, vits_afrotts_ft_ext_spk_audio_path_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and confidence interval for each array\n",
    "mean_ci_afrotts, ci_afrotts = compute_confidence_interval(vits_afrotts_cos_sim_seen)\n",
    "mean_ci_afrotts_ft, ci_afrotts_ft = compute_confidence_interval(vits_afrotts_ft_cos_sim_seen)\n",
    "mean_ci_afrotts_ft_ext_spk_seen, ci_afrotts_ft_ext_spk_seen = compute_confidence_interval(vits_afrotts_ft_ext_spk_cos_sim_seen)\n",
    "mean_ci_afrotts_ft_ext_spk_unseen, ci_afrotts_ft_ext_spk_unseen = compute_confidence_interval(vits_afrotts_ft_ext_spk_cos_sim_unseen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Model': [\n",
    "        'VITS AfriTTS Seen',\n",
    "        'VITS AfriTTS FT Seen',\n",
    "        'VITS AfriTTS FT EXT SPK Seen',\n",
    "        'VITS AfriTTS FT EXT SPK Unseen'\n",
    "    ],\n",
    "    'Mean Cosine Similarity': [\n",
    "        mean_ci_afrotts,\n",
    "        mean_ci_afrotts_ft,\n",
    "        mean_ci_afrotts_ft_ext_spk_seen,\n",
    "        mean_ci_afrotts_ft_ext_spk_unseen\n",
    "    ],\n",
    "    'Confidence Interval (CI)': [\n",
    "        ci_afrotts,\n",
    "        ci_afrotts_ft,\n",
    "        ci_afrotts_ft_ext_spk_seen,\n",
    "        ci_afrotts_ft_ext_spk_unseen\n",
    "    ]\n",
    "}\n",
    "cos_sim_results_df = pd.DataFrame(data)\n",
    "display(cos_sim_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WV-MOS (Overall quality)\n",
    "\n",
    "https://github.com/AndreevP/wvmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/AndreevP/wvmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# ATTENTION\n",
    "#\n",
    "# you need a gpu to load the model\n",
    "# =======================\n",
    "from wvmos import get_wvmos\n",
    "\n",
    "wvmos_model = get_wvmos(cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mos_array = []\n",
    "\n",
    "# for _, wav_file_tts in path_to_wavs:\n",
    "#     mos_audio = wvmos_model.calculate_one(wav_file_tts) # infer MOS score for one audio\n",
    "    \n",
    "#     mos_array.append(mos_audio)\n",
    "    \n",
    "# mean_mos, ci_mos = compute_confidence_interval(mos_array)\n",
    "# #report the values, mean +/- ci\n",
    "# print(f\"model mcd score: {mean_mos} + {ci_mos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if a TTSmodel is statistically better than another TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # e.g., To verify that model 1 is better than model 2 in WV-mos scores\n",
    "\n",
    "# diff_in_scores = mos_score_array_of_model1 - mos_score_array_of_model2\n",
    "\n",
    "\n",
    "# mean_score, ci = compute_confidence_interval(diff_in_scores,)\n",
    "\n",
    "# # If the confidence intervals lie fully on the positive side on the real axis, \n",
    "# # this means that the difference is statistically significant. \n",
    "# # E.g., for WV-MOS, the confidence interval will be 0.14 +/- 0.xx. If xx is smaller than 14, \n",
    "# # then the difference is statistically significant.\n",
    "\n",
    "# if mean_score - ci > 0:\n",
    "#     print(\"model A is better than model B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
