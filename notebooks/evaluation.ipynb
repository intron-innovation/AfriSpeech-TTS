{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVKWj1ZOSkJD"
   },
   "source": [
    "## TTS model evaluation (VITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POSw5dzUSkJE"
   },
   "outputs": [],
   "source": [
    "!pip install awscli\n",
    "!pip install --upgrade huggingface_hub\n",
    "\n",
    "!pip install 'transformers[torch]'\n",
    "!pip install 'datasets[audio]'\n",
    "\n",
    "!pip install gradio\n",
    "!pip install --upgrade evaluate jiwer\n",
    "# !pip install mel-cepstral-distance --user\n",
    "!pip install resemblyzer\n",
    "!pip install git+https://github.com/AndreevP/wvmos\n",
    "!git clone https://github.com/gabrielmittag/NISQA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1c7OjxLSkJF"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from datasets import Dataset\n",
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    VitsModel,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG_Q5ZrkSkJF"
   },
   "source": [
    "### Dataset & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-Q-sgn1WFv3"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \".\" # change to your data root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTGkg_ZlWR22"
   },
   "outputs": [],
   "source": [
    "# # Download the audio data (if not already downloaded)\n",
    "\n",
    "# !aws configure\n",
    "# !aws s3 cp s3://intron-open-source/AfriSpeech-TTS-D {DATA_DIR}/AfriSpeech-TTS-D/ --recursive\n",
    "# !aws s3 cp s3://intron-open-source/AfriSpeech-TTS/tts_generated_speech {DATA_DIR}/AfriSpeech-TTS-D/tts_generated_speech/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qc2YR8jSkJF"
   },
   "outputs": [],
   "source": [
    "# load test_seen csv file\n",
    "test_seen = pd.read_csv(\"https://raw.githubusercontent.com/intron-innovation/AfriSpeech-TTS/vits/data/afritts-test-seen-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnxYj4ZpgSNM",
    "outputId": "0355af2f-8f58-41bd-ccd1-ca080c3c0b2a"
   },
   "outputs": [],
   "source": [
    "def load_data(prefix_path, file):\n",
    "    file_path = os.path.join(prefix_path, file + '.txt')\n",
    "    df = pd.read_csv(file_path, sep='|', header=None)\n",
    "    expected_columns = ['audio_path', 'text', 'country', 'accent', 'speaker_id', 'sentence_id']\n",
    "    if not set(expected_columns).issubset(set(df.iloc[0])):\n",
    "        df.columns = expected_columns\n",
    "    else:\n",
    "        df = pd.read_csv(file_path, sep='|')\n",
    "\n",
    "    # Append the specified prefix to each entry in the 'audio_path' column\n",
    "    df['audio_path'] = df['audio_path'].apply(lambda x: os.path.join(prefix_path, os.path.basename(os.path.dirname(x)), os.path.basename(x)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_audio_texts(df):\n",
    "    # Extract 'audio_path' and 'text' columns, convert to a list of tuples\n",
    "    audio_texts = list(df[['audio_path', 'text']].itertuples(index=False, name=None))\n",
    "    return audio_texts\n",
    "\n",
    "TTS_DIR = f\"{DATA_DIR}/AfriSpeech-TTS-D/tts_generated_speech/afritts_test_seen\"\n",
    "tts_generated_folders = [\n",
    "    'vits_afrotts',\n",
    "    'vits_afrotts_ft',\n",
    "    'vits_afrotts_ft_ext_spk',\n",
    "    'vits_vctk',\n",
    "    'xtts',\n",
    "    'xtts_ft'\n",
    "]\n",
    "\n",
    "audio_texts_dict = {}\n",
    "\n",
    "for tts_model in tts_generated_folders:\n",
    "    df = load_data(TTS_DIR, tts_model) \n",
    "    audio_texts = extract_audio_texts(df) \n",
    "    audio_texts_dict[file.split('.')[0]] = audio_texts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGscsn3OSkJG"
   },
   "outputs": [],
   "source": [
    "# explore what the speech utterances sound like\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def display_audio(audio_texts_dict, key, indices):\n",
    "    with gr.Blocks() as demo:\n",
    "      with gr.Column():\n",
    "          for idx in indices:\n",
    "              audio, label = audio_texts_dict[key][idx][0], audio_texts_dict[key][idx][1]\n",
    "              output = gr.Audio(audio, label=label)\n",
    "    demo.launch(debug=False)\n",
    "\n",
    "# select random examples\n",
    "indices = np.random.choice(700, 1, replace=False)\n",
    "print(indices)\n",
    "\n",
    "for key in audio_texts_dict.keys():\n",
    "  print(f\"Displaying audio for {key}\")\n",
    "  display_audio(audio_texts_dict, key, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19qdRQmESkJG",
    "tags": []
   },
   "source": [
    "### Word Error Rate (Intelligibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alukHsMeSkJG"
   },
   "outputs": [],
   "source": [
    "# Load ASR model\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "asr_model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "asr_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    asr_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True)\n",
    "asr_model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(asr_model_id)\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=asr_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs={\"language\": \"english\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yh9ypzSKSkJG"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def load_audio_file(audio_path, sampling_rate=16000):\n",
    "    audio, _ = librosa.load(audio_path, sr=sampling_rate)\n",
    "    return audio\n",
    "\n",
    "\n",
    "# Transcribe speech utterances using ASR model\n",
    "\n",
    "def transcribe(model, audio_texts_tuples, batch_size=16):\n",
    "    transcriptions = []\n",
    "    audio_paths = [audio_text[0] for audio_text in audio_texts_tuples]\n",
    "    all_audio_data = Parallel(n_jobs=-1)(\n",
    "        delayed(load_audio_file)(path) for path in tqdm(audio_paths, desc=\"Loading audio files\")\n",
    "    )\n",
    "    for i in tqdm(range(0, len(all_audio_data), batch_size), desc=\"Transcribing batches\"):\n",
    "        batch_audio_data = all_audio_data[i:i+batch_size]\n",
    "        results = model(batch_audio_data)\n",
    "        batch_transcriptions = [result[\"text\"] for result in results]\n",
    "        transcriptions.extend(batch_transcriptions)\n",
    "    return transcriptions\n",
    "\n",
    "transcriptions_dict = {}\n",
    "\n",
    "for key in audio_texts_dict.keys():\n",
    "  transcriptions_dict[key] = transcribe(asr_pipe, audio_texts_dict[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "5MpYIGvcSkJG",
    "outputId": "4ffffc3c-1831-49cd-f463-6d99eca6b735"
   },
   "outputs": [],
   "source": [
    "# Compute WER for the transcriptions\n",
    "\n",
    "import evaluate\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "def compute_normalized_wer(predictions, ground_truth):\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "    # Normalize predictions and ground truth\n",
    "    normalizer = BasicTextNormalizer()\n",
    "    predictions_norm = [normalizer(pred) for pred in predictions]\n",
    "    references_norm = [normalizer(label) for label in ground_truth]\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(references=references_norm, predictions=predictions_norm)\n",
    "\n",
    "    return wer\n",
    "\n",
    "wer_dict = {}\n",
    "\n",
    "# Compute WER for each key in the dictionary\n",
    "for key in transcriptions_dict.keys():\n",
    "  ground_truth_list = [text for _, text in audio_texts_dict[key]]\n",
    "  predictions_list = transcriptions_dict[key]\n",
    "  wer_dict[key] = compute_normalized_wer(predictions_list, ground_truth_list)\n",
    "\n",
    "wer_df = pd.DataFrame(list(wer_dict.items()), columns=['Model', 'WER'])\n",
    "display(wer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtba-YHWSkJG"
   },
   "source": [
    "### Mel-Cepstral-Distance (Speech Signal Similarity)\n",
    "https://github.com/jasminsternkopf/mel_cepstral_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PchQamhJSkJG"
   },
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from mel_cepstral_distance import get_metrics_wavs\n",
    "\n",
    "# def compute_mcd(ref_audio_path, synth_audio_path):\n",
    "#     mcd_arr = []\n",
    "#     for ref, synth in zip(ref_audio_path, synth_audio_path):\n",
    "#         mcd_audio, _, _ = get_metrics_wavs(Path(ref), Path(synth), use_dtw=False)\n",
    "#         mcd_arr.append(mcd_audio)\n",
    "#     return mcd_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOtgAWA3Ng_o"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# function to compute confidence interval\n",
    "\n",
    "def compute_confidence_interval(score_arr, ci=0.95):\n",
    "    # computes the CI at 95 perc confidence level\n",
    "\n",
    "    # Filter out NaN values\n",
    "    score_arr = np.array(score_arr)\n",
    "    clean_arr = score_arr[~np.isnan(score_arr)]\n",
    "    mean_score = np.nanmean(clean_arr)\n",
    "    ci = st.t.interval(\n",
    "            confidence=ci,\n",
    "            df=len(clean_arr) - 1,\n",
    "            loc=mean_score,\n",
    "            scale=st.sem(clean_arr) if np.std(clean_arr) > 0 else 0,\n",
    "        )\n",
    "\n",
    "    return mean_score, mean_score-ci[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zoe3gRbrSkJG"
   },
   "source": [
    "### Cosine Distance (Speaker Similarity)\n",
    "https://github.com/resemble-ai/Resemblyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJkQgWkLSkJH",
    "outputId": "3faa772e-8971-42d7-924b-dd87c55079bc"
   },
   "outputs": [],
   "source": [
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_cos_sim(encoder, ref_audio_path, synth_audio_path):\n",
    "    cos_sim_arr = []\n",
    "    for ref, synth in zip(ref_audio_path, synth_audio_path):\n",
    "            ref_wav = preprocess_wav(Path(ref))\n",
    "            gen_wav = preprocess_wav(Path(synth))\n",
    "\n",
    "            ref_emb = encoder.embed_utterance(ref_wav)\n",
    "            gen_emb = encoder.embed_utterance(gen_wav)\n",
    "\n",
    "            # the embeddings are already l2 normalized by the speaker model\n",
    "            cos_sim = ref_emb @ gen_emb\n",
    "\n",
    "            cos_sim_arr.append(cos_sim)\n",
    "    return cos_sim_arr\n",
    "\n",
    "encoder = VoiceEncoder()\n",
    "\n",
    "cos_sim_dict = {}\n",
    "\n",
    "# Compute cosine similarity for each key in the dictionary\n",
    "for key in audio_texts_dict.keys():\n",
    "  synth_audio_path = [path for path, _ in audio_texts_dict[key]]\n",
    "  ref_audio_path = [f\"{DATA_DIR}{path}\" for path in test_seen['audio_paths']]\n",
    "  cos_sim_dict[key] = compute_cos_sim(encoder, ref_audio_path, synth_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "LygwLAYuJNLv",
    "outputId": "b1d64823-4108-4217-b13e-f925682ae55a"
   },
   "outputs": [],
   "source": [
    "ci_results = {}\n",
    "\n",
    "# Compute the confidence interval for each model\n",
    "for model, scores in cos_sim_dict.items():\n",
    "    mean_score, ci = compute_confidence_interval(scores)\n",
    "    ci_results[model] = {'Mean Cosine Similarity': mean_score, 'Confidence Interval': ci}\n",
    "\n",
    "ci_df = pd.DataFrame.from_dict(ci_results, orient='index')\n",
    "display(ci_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eqTsi35SkJH"
   },
   "source": [
    "### WV-MOS (Overall quality)\n",
    "\n",
    "https://github.com/AndreevP/wvmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sctPhDevSkJH"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# you need a gpu to load the model\n",
    "# =======================\n",
    "from wvmos import get_wvmos\n",
    "\n",
    "wvmos_model = get_wvmos(cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azcetvzYpb9J"
   },
   "outputs": [],
   "source": [
    "def compute_mos_scores(audio_paths, model):\n",
    "    mos_scores = []\n",
    "    for wav_file_path in audio_paths:\n",
    "        # Infer MOS score for one audio file\n",
    "        mos_score = model.calculate_one(wav_file_path)\n",
    "        mos_scores.append(mos_score)\n",
    "\n",
    "    return mos_scores\n",
    "\n",
    "\n",
    "wvmos_dict = {}\n",
    "\n",
    "# Compute WV-MOS for each key in the dictionary\n",
    "for key in audio_texts_dict.keys():\n",
    "  audio_paths = [path for path, _ in audio_texts_dict[key]]\n",
    "  wvmos_dict[key] = compute_mos_scores(audio_paths, wvmos_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "ZyZ39Vb6uQ2F",
    "outputId": "644ec13a-28e4-422b-ee20-756c85fb00db"
   },
   "outputs": [],
   "source": [
    "ci_results = {}\n",
    "\n",
    "# Compute the confidence interval for each model\n",
    "for model, scores in wvmos_dict.items():\n",
    "    mean_score, ci = compute_confidence_interval(scores)\n",
    "    ci_results[model] = {'Mean WV-MOS': mean_score, 'Confidence Interval': ci}\n",
    "\n",
    "ci_df = pd.DataFrame.from_dict(ci_results, orient='index')\n",
    "display(ci_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW78sPb1OSQI"
   },
   "source": [
    "### NISQA (Speech Quality and Naturalness Assessment)\n",
    "https://github.com/gabrielmittag/NISQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpdnSFFtOK0s"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output_csv = f\"{DATA_DIR}/NISQA_results.csv\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for tts_model in tts_generated_folders:\n",
    "    dir = os.path.join(TTS_DIR, tts_model)\n",
    "    command = f'python NISQA/run_predict.py --mode predict_dir --pretrained_model NISQA/weights/nisqa.tar --data_dir {dir} --num_workers 0 --bs 10  --output_dir {DATA_DIR}'\n",
    "    subprocess.run(command, shell=True, check=True)\n",
    "    df = pd.read_csv(output_csv)\n",
    "    df['Model'] = os.path.basename(tts_model)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "wpGExIM3tZlP",
    "outputId": "04da8115-6522-42ac-9b91-4059848d9ea6"
   },
   "outputs": [],
   "source": [
    "ci_results_df = pd.DataFrame()\n",
    "\n",
    "score_columns = ['mos_pred', 'noi_pred', 'dis_pred', 'col_pred', 'loud_pred']\n",
    "\n",
    "for model, group in combined_df.groupby('Model'):\n",
    "    ci_results = {'Model': model}\n",
    "    for col in score_columns:\n",
    "        mean_score, ci = compute_confidence_interval(group[col])\n",
    "        ci_results[f'{col}_mean'] = mean_score\n",
    "        ci_results[f'{col}_ci'] = ci\n",
    "    ci_results_df = ci_results_df.append(ci_results, ignore_index=True)\n",
    "\n",
    "display(ci_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZN5WuwZSkJH"
   },
   "source": [
    "### check if a TTSmodel is statistically better than another TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VLq8aAH2snM"
   },
   "outputs": [],
   "source": [
    "# # verify that model 1 is better than model 2 in WV-mos scores\n",
    "\n",
    "# def compare_models(mos_scores_model1, mos_scores_model2, ci_level=0.95):\n",
    "#     # Compute difference in scores\n",
    "#     diff_in_scores = np.array(mos_scores_model1) - np.array(mos_scores_model2)\n",
    "\n",
    "#     # Compute the confidence interval of the difference\n",
    "#     mean_score, ci_half_width = compute_confidence_interval(diff_in_scores, ci=ci_level)\n",
    "#     lower_bound = mean_score - ci_half_width\n",
    "#     upper_bound = mean_score + ci_half_width\n",
    "\n",
    "#     # If the confidence intervals lie fully on the positive side on the real axis,\n",
    "#     # this means that the difference is statistically significant.\n",
    "#     # E.g., for WV-MOS, the confidence interval will be 0.14 +/- 0.xx. If xx is smaller than 14,\n",
    "#     # then the difference is statistically significant.\n",
    "\n",
    "#     # Check if the confidence interval lies fully on the positive side\n",
    "#     if lower_bound > 0:\n",
    "#         print(\"Model 1 is statistically significantly better than Model 2\")\n",
    "#     elif upper_bound < 0:\n",
    "#         print(\"Model 2 is statistically significantly better than Model 1\")\n",
    "#     else:\n",
    "#         print(\"No statistically significant difference between Model 1 and Model 2\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
