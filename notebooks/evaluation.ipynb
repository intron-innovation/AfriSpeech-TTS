{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS model evaluation (VITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules \n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from datasets import Dataset\n",
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    VitsModel,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data & Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv files\n",
    "\n",
    "DATA_ROOT = \"../data\"\n",
    "test_seen = pd.read_csv(f\"{DATA_ROOT}/afritts-test-seen-clean.csv\")\n",
    "test_unseen = pd.read_csv(f\"{DATA_ROOT}/afritts-test-unseen-clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom HF dataset\n",
    "\n",
    "# Convert DataFrame to a dictionary with lists\n",
    "test_seen['age_group'] = test_seen['age_group'].astype(str).fillna('null')\n",
    "test_seen_dict = test_seen.to_dict(orient='list')\n",
    "test_unseen_dict = test_unseen.to_dict(orient='list')\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "seen_dataset = Dataset.from_dict(test_seen_dict)\n",
    "unseen_dataset = Dataset.from_dict(test_unseen_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f777746322344f2a9464df9ec7035d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select a subset of the seen dataset\n",
    "seen_dataset_selected = seen_dataset.filter(lambda example: example['gender'] in ['Male', 'Female']).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mms-tts-eng were not used when initializing VitsModel: ['posterior_encoder.wavenet.res_skip_layers.8.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_g', 'posterior_encoder.wavenet.res_skip_layers.2.weight_v', 'flow.flows.2.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_g', 'flow.flows.3.wavenet.res_skip_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_g', 'flow.flows.3.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.13.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.9.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_g', 'flow.flows.0.wavenet.res_skip_layers.2.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.4.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_v', 'flow.flows.3.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.7.weight_v', 'flow.flows.2.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.res_skip_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.2.weight_g', 'flow.flows.0.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.4.weight_v', 'posterior_encoder.wavenet.in_layers.14.weight_g', 'flow.flows.2.wavenet.in_layers.3.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.0.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_g', 'flow.flows.0.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_g', 'posterior_encoder.wavenet.res_skip_layers.7.weight_v', 'posterior_encoder.wavenet.res_skip_layers.4.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.6.weight_v', 'flow.flows.2.wavenet.in_layers.2.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_v', 'posterior_encoder.wavenet.res_skip_layers.10.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.10.weight_g', 'posterior_encoder.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_g', 'flow.flows.2.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_g', 'flow.flows.0.wavenet.in_layers.3.weight_v', 'flow.flows.1.wavenet.in_layers.1.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.10.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.13.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_v', 'flow.flows.1.wavenet.res_skip_layers.2.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.15.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.1.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.13.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_g', 'posterior_encoder.wavenet.in_layers.14.weight_v', 'flow.flows.3.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.2.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.0.weight_g', 'flow.flows.2.wavenet.in_layers.0.weight_g', 'flow.flows.2.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_g', 'flow.flows.0.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.11.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_v', 'posterior_encoder.wavenet.in_layers.4.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.6.weight_g', 'flow.flows.1.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_g', 'flow.flows.0.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.10.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.15.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_v', 'flow.flows.1.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_v', 'flow.flows.1.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_v', 'posterior_encoder.wavenet.res_skip_layers.7.weight_g', 'flow.flows.2.wavenet.in_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.11.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_g']\n",
      "- This IS expected if you are initializing VitsModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VitsModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VitsModel were not initialized from the model checkpoint at facebook/mms-tts-eng and are newly initialized: ['flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load TTS models and tokenizer\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Initialize the base model\n",
    "base_model_id = \"facebook/mms-tts-eng\"\n",
    "base_model = VitsModel.from_pretrained(base_model_id, torch_dtype=torch_dtype, use_safetensors=True)\n",
    "base_model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "# Initialize the finetuned model\n",
    "# finetuned_model_id = \"your_finetuned_model_id_here\"\n",
    "# finetuned_model = VitsModel.from_pretrained(finetuned_model_id)\n",
    "# finetuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synthesizing speech - base_unseen: 100%|████████| 16/16 [00:24<00:00,  1.51s/it]\n",
      "Synthesizing speech - base_seen: 100%|██████████| 10/10 [00:12<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run Inference and Save Speech Utterances\n",
    "\n",
    "def synthesize_save(model, tokenizer, texts, output_dir, prefix=\"\"):\n",
    "    set_seed(555)  # make deterministic\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    synthesized_speech = []\n",
    "    for i, text in tqdm(enumerate(texts), total=len(texts), desc=f\"Synthesizing speech - {prefix}\"):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        audio = outputs.waveform[0].numpy()\n",
    "        file_path = os.path.join(output_dir, f\"{prefix}_utterance_{i}.wav\")\n",
    "        scipy.io.wavfile.write(file_path, rate=model.config.sampling_rate, data=audio)\n",
    "        synthesized_speech.append(file_path)\n",
    "    return synthesized_speech\n",
    "\n",
    "output_dir = f\"{DATA_ROOT}/AfriSpeech-TTS-D/tts_generated_speech/\"\n",
    "\n",
    "# Run for unseen dataset\n",
    "gt_transcripts_unseen = [example['transcript'] for example in unseen_dataset]\n",
    "gt_speech_unseen = [f\"{DATA_ROOT}\" + example['audio_paths'] for example in unseen_dataset]\n",
    "base_speech_unseen = synthesize_save(base_model, tokenizer, gt_transcripts_unseen, output_dir, \"base_unseen\")\n",
    "# ft_speech_unseen = synthesize_save(finetuned_model, tokenizer, gt_transcripts_unseen, output_dir, \"ft_unseen\")\n",
    "\n",
    "# Run for seen dataset\n",
    "gt_transcripts_seen = [example['transcript'] for example in seen_dataset_selected]\n",
    "gt_speech_seen = [f\"{DATA_ROOT}\" + example['audio_paths'] for example in seen_dataset_selected]\n",
    "base_speech_seen = synthesize_save(base_model, tokenizer, gt_transcripts_seen, output_dir, \"base_seen\")\n",
    "# ft_speech_seen = synthesize_save(finetuned_model, tokenizer, gt_transcripts_seen, output_dir, \"ft_seen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: \n",
      "\n",
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VITS (eng) baseline: \n",
      "\n",
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_audio(audio_files, transcripts, n=1):\n",
    "    with gr.Blocks() as demo:\n",
    "      with gr.Column():\n",
    "        for i in range(n):\n",
    "            audio, label = audio_files[i], transcripts[i]\n",
    "            output = gr.Audio(audio, label=label)    \n",
    "    demo.launch(debug=False)\n",
    "    \n",
    "print(\"Ground Truth: \\n\")\n",
    "display_audio(gt_speech_seen, gt_transcripts_seen)\n",
    "\n",
    "print(\"VITS (eng) baseline: \\n\")\n",
    "display_audio(base_speech_seen, gt_transcripts_seen)\n",
    "\n",
    "# print(\"VITS (eng) FT: \\n\")\n",
    "# display_audio(ft_speech_seen, gt_transcripts_seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word Error Rate (Intelligibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load ASR model (Whisper)\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "asr_model_id = \"openai/whisper-medium\" #open_ai/whisper-medium-general\n",
    "\n",
    "asr_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    asr_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "asr_model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(asr_model_id)\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=asr_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing audio files: 100%|█████████████████| 16/16 [01:31<00:00,  5.72s/it]\n",
      "Transcribing audio files: 100%|█████████████████| 16/16 [01:25<00:00,  5.32s/it]\n",
      "Transcribing audio files: 100%|█████████████████| 10/10 [00:51<00:00,  5.11s/it]\n",
      "Transcribing audio files: 100%|█████████████████| 10/10 [00:51<00:00,  5.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Transcribe speech utterances using ASR \n",
    "\n",
    "def transcribe(model, audio_files):\n",
    "    transcriptions = []\n",
    "    for audio_path in tqdm(audio_files, desc=\"Transcribing audio files\"):\n",
    "        audio, sampling_rate = librosa.load(audio_path)\n",
    "        result = model(audio)\n",
    "        transcriptions.append(result[\"text\"])\n",
    "    return transcriptions\n",
    "\n",
    "# Run for unseen dataset\n",
    "asr_transcripts_unseen = transcribe(asr_pipe, gt_speech_unseen)\n",
    "base_transcripts_unseen = transcribe(asr_pipe, base_speech_unseen)\n",
    "# ft_transcripts_unseen = transcribe(asr_model, ft_speech_unseen)\n",
    "\n",
    "\n",
    "# Run for seen dataset\n",
    "asr_transcripts_seen = transcribe(asr_pipe, gt_speech_seen)\n",
    "base_transcripts_seen = transcribe(asr_pipe, base_speech_seen)\n",
    "# ft_transcripts_seen = transcribe(asr_model, ft_speech_seen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute WER for the Transcriptions\n",
    "\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "def compute_normalized_wer(predictions, ground_truth):\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    \n",
    "    # Normalize predictions and ground truth\n",
    "    normalizer = BasicTextNormalizer()\n",
    "    predictions_norm = [normalizer(pred) for pred in predictions]\n",
    "    references_norm = [normalizer(label) for label in ground_truth]\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(references=references_norm, predictions=predictions_norm)\n",
    "\n",
    "    return wer\n",
    "\n",
    "\n",
    "# Run for unseen dataset\n",
    "gt_wer_unseen = compute_normalized_wer(asr_transcripts_unseen, gt_transcripts_unseen)\n",
    "base_wer_unseen = compute_normalized_wer(base_transcripts_unseen, gt_transcripts_unseen)\n",
    "# ft_wer_unseen = compute_normalized_wer(ft_transcripts_unseen, gt_transcripts_unseen)\n",
    "\n",
    "# Run for seen dataset\n",
    "gt_wer_seen = compute_normalized_wer(asr_transcripts_seen, gt_transcripts_seen)\n",
    "base_wer_seen = compute_normalized_wer(base_transcripts_seen, gt_transcripts_seen)\n",
    "# ft_wer_seen = compute_normalized_wer(ft_transcripts_seen, gt_transcripts_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>WER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ground Truth</td>\n",
       "      <td>afritts-test-unseen-clean</td>\n",
       "      <td>0.432665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VITS (eng) baseline</td>\n",
       "      <td>afritts-test-unseen-clean</td>\n",
       "      <td>0.392550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VITS (eng) FT</td>\n",
       "      <td>afritts-test-unseen-clean</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ground Truth</td>\n",
       "      <td>afritts-test-seen-clean</td>\n",
       "      <td>0.483146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VITS (eng) baseline</td>\n",
       "      <td>afritts-test-seen-clean</td>\n",
       "      <td>0.584270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VITS (eng) FT</td>\n",
       "      <td>afritts-test-seen-clean</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Utterance                    Dataset       WER\n",
       "0         Ground Truth  afritts-test-unseen-clean  0.432665\n",
       "1  VITS (eng) baseline  afritts-test-unseen-clean  0.392550\n",
       "2        VITS (eng) FT  afritts-test-unseen-clean  1.000000\n",
       "3         Ground Truth    afritts-test-seen-clean  0.483146\n",
       "4  VITS (eng) baseline    afritts-test-seen-clean  0.584270\n",
       "5        VITS (eng) FT    afritts-test-seen-clean  1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ft_wer_unseen, ft_wer_seen = 1.0, 1.0\n",
    "\n",
    "# Compile the results\n",
    "results = {\n",
    "    \"Utterance\": [\"Ground Truth\", \"VITS (eng) baseline\", \"VITS (eng) FT\", \"Ground Truth\", \"VITS (eng) baseline\", \"VITS (eng) FT\"],\n",
    "    \"Dataset\": [\"afritts-test-unseen-clean\", \"afritts-test-unseen-clean\", \"afritts-test-unseen-clean\", \"afritts-test-seen-clean\", \"afritts-test-seen-clean\", \"afritts-test-seen-clean\"],\n",
    "    \"WER\": [gt_wer_unseen, base_wer_unseen, ft_wer_unseen, gt_wer_seen, base_wer_seen, ft_wer_seen]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_results.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WV-MOS (Overall quality)\n",
    "\n",
    "https://github.com/AndreevP/wvmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/AndreevP/wvmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute confidence interval\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "def compute_confidence_interval(score_arr, ci=0.95):\n",
    "    # computes the CI at 95 perc confidence level\n",
    "    \n",
    "    mean_score = np.mean(score_arr)\n",
    "    ci = st.t.interval(\n",
    "            alpha=0.95,\n",
    "            df=len(score_arr) - 1,\n",
    "            loc=mean_score,\n",
    "            scale=st.sem(score_arr),\n",
    "        )\n",
    "    \n",
    "    return mean_score, mean_score-ci[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need a gpu to load the model\n",
    "from wvmos import get_wvmos\n",
    "\n",
    "wvmos_model = get_wvmos(cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mos_array = []\n",
    "\n",
    "for _, wav_file_tts in path_to_wavs:\n",
    "    mos_audio = wvmos_model.calculate_one(wav_file_tts) # infer MOS score for one audio\n",
    "    \n",
    "    mos_array.append(mos_audio)\n",
    "    \n",
    "mean_mos, ci_mos = compute_confidence_interval(mos_array)\n",
    "#report the values, mean +/- ci\n",
    "print(f\"model mcd score: {mean_mos} + {ci_mos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-Cepstral-Distance (Speech Signal Similarity)\n",
    "https://github.com/jasminsternkopf/mel_cepstral_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mel_cepstral_distance import get_metrics_wavs, get_metrics_mels, get_metrics_mels_pairwise\n",
    "\n",
    "mcd_arr = []\n",
    "\n",
    "for wav_file_ref, wav_file_tts in path_to_wavs:\n",
    "    mcd_audio, _, _ = get_metrics_wavs(wav_file_ref, wav_file_tts,)\n",
    "    \n",
    "    mcd_arr.append(mcd_audio)\n",
    "    \n",
    "\n",
    "mean_mcd, ci_mcd = compute_confidence_interval(mcd_arr)\n",
    "#report the values, mean +/- ci\n",
    "print(f\"model mcd score: {mean_mcd} + {ci_mcd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance (Speaker Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install resemblyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resemblyzer import VoiceEncoder, preprocess_wav\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "encoder = VoiceEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_arr = [] \n",
    "\n",
    "for wav_file_ref, wav_file_tts in path_to_wavs:\n",
    "        ref_wav = preprocess_wav(Path(wav_file_ref))\n",
    "        gen_wav = preprocess_wav(Path(wav_file_tts))\n",
    "        \n",
    "        ref_emb = encoder.embed_utterance(ref_wav)\n",
    "        gen_emb = encoder.embed_utterance(gen_wav)\n",
    "        \n",
    "        # the embeddings are already l2 normalized by the speaker model\n",
    "        cos_sim = ref_emb @ gen_emb\n",
    "        \n",
    "        cos_sim_arr.append(cos_sim)\n",
    "        \n",
    "mean_cos_sim, ci_cos_sim = compute_confidence_interval(cos_sim_arr)\n",
    "#report the values, mean +/- ci\n",
    "print(f\"model mcd score: {mean_cos_sim} + {ci_cos_sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if a TTSmodel is statistically better than another TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# e.g., To verify that model 1 is better than model 2 in WV-mos scores\n",
    "\n",
    "diff_in_scores = mos_score_array_of_model1 - mos_score_array_of_model2\n",
    "\n",
    "\n",
    "mean_score, ci = compute_confidence_interval(diff_in_scores,)\n",
    "\n",
    "# If the confidence intervals lie fully on the positive side on the real axis, \n",
    "# this means that the difference is statistically significant. \n",
    "# E.g., for WV-MOS, the confidence interval will be 0.14 +/- 0.xx. If xx is smaller than 14, \n",
    "# then the difference is statistically significant.\n",
    "\n",
    "if mean_score - ci > 0:\n",
    "    print(\"model A is better than model B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
